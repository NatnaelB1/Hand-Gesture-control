{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d405ba18",
   "metadata": {},
   "source": [
    "## Hand Gesture Controls For YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18770b",
   "metadata": {},
   "source": [
    "The Hand Gesture program is developed using Jupyter Notebook and implemented in Python 3. \n",
    "Make sure to install required libraries for the project:\n",
    "    \n",
    "    - Numpy:     !pip install numpy\n",
    "    - OpenCV:    !pip install opencv-python \n",
    "    - Mediapipe: !pip install mediapipe\n",
    "    - Pyautogui: !pip install PyAutoGUI\n",
    "    - Time:      !pip install python-time\n",
    "    - Sys:       !pip install numpy\n",
    "\n",
    "The hand detection algorithm is written for the right hand, with the palm facing the camera. After successfully running the program, make sure the Browser window containing YouTube is focused on since the program automates keyboard presses based on the different gestures. The media player controls include Pause/Play, Skip video, Volume up/Volume down, Mute, Seek left/right, and Full screen control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5bc345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natnaelalemu/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# For drawing the hand landmarks and connections\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# For detecting hands and their landmarks\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# initializes a VideoCapture (change value to 0 depending of the camera used)\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# set the initial time to the current time\n",
    "last_press_time = time.time()   \n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "# Helper function, that determines the current gesture from the given hand_landmarks\n",
    "def Gesture_recognizer(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        hand_landmarks: landmarks from mediapipe\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    global last_press_time\n",
    "    # finger coordinates\n",
    "    # Thumb coordinates\n",
    "    thumb_tip = np.array([hand_landmarks[mp_hands.HandLandmark.THUMB_TIP.value].x, hand_landmarks[mp_hands.HandLandmark.THUMB_TIP.value].y])\n",
    "    thumb_ip = np.array([hand_landmarks[mp_hands.HandLandmark.THUMB_IP.value].x, hand_landmarks[mp_hands.HandLandmark.THUMB_IP.value].y])\n",
    "    thumb_mcp = np.array([hand_landmarks[mp_hands.HandLandmark.THUMB_MCP.value].x, hand_landmarks[mp_hands.HandLandmark.THUMB_MCP.value].y])\n",
    "    thumb_cmc = np.array([hand_landmarks[mp_hands.HandLandmark.THUMB_CMC.value].x, hand_landmarks[mp_hands.HandLandmark.THUMB_CMC.value].y])\n",
    "     \n",
    "    # Index finger coordinates\n",
    "    index_tip = np.array([hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP.value].x, hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP.value].y])\n",
    "    index_dip = np.array([hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_DIP.value].x, hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_DIP.value].y])\n",
    "    index_pip = np.array([hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_PIP.value].x, hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_PIP.value].y])\n",
    "    index_mcp = np.array([hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_MCP.value].x, hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_MCP.value].y])\n",
    "\n",
    "    # Middle finger coordinates\n",
    "    middle_tip = np.array([hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP.value].x, hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP.value].y])\n",
    "    middle_dip = np.array([hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_DIP.value].x, hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_DIP.value].y])\n",
    "    middle_pip = np.array([hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_PIP.value].x, hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_PIP.value].y])\n",
    "    middle_mcp = np.array([hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_MCP.value].x, hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_MCP.value].y])\n",
    "    \n",
    "    # ring finger coordinates\n",
    "    ring_tip = np.array([hand_landmarks[mp_hands.HandLandmark.RING_FINGER_TIP.value].x, hand_landmarks[mp_hands.HandLandmark.RING_FINGER_TIP.value].y])\n",
    "    ring_dip = np.array([hand_landmarks[mp_hands.HandLandmark.RING_FINGER_DIP.value].x, hand_landmarks[mp_hands.HandLandmark.RING_FINGER_DIP.value].y])\n",
    "    ring_pip = np.array([hand_landmarks[mp_hands.HandLandmark.RING_FINGER_PIP.value].x, hand_landmarks[mp_hands.HandLandmark.RING_FINGER_PIP.value].y])\n",
    "    ring_mcp = np.array([hand_landmarks[mp_hands.HandLandmark.RING_FINGER_MCP.value].x, hand_landmarks[mp_hands.HandLandmark.RING_FINGER_MCP.value].y])\n",
    "    \n",
    "    # pinky finger coordinates\n",
    "    pinky_tip = np.array([hand_landmarks[mp_hands.HandLandmark.PINKY_TIP.value].x, hand_landmarks[mp_hands.HandLandmark.PINKY_TIP.value].y])\n",
    "    pinky_dip = np.array([hand_landmarks[mp_hands.HandLandmark.PINKY_DIP.value].x, hand_landmarks[mp_hands.HandLandmark.PINKY_DIP.value].y])\n",
    "    pinky_pip = np.array([hand_landmarks[mp_hands.HandLandmark.PINKY_PIP.value].x, hand_landmarks[mp_hands.HandLandmark.PINKY_PIP.value].y])\n",
    "    pinky_mcp = np.array([hand_landmarks[mp_hands.HandLandmark.PINKY_MCP.value].x, hand_landmarks[mp_hands.HandLandmark.PINKY_MCP.value].y])\n",
    "    \n",
    "    # wrist coordinates\n",
    "    wrist = np.array([hand_landmarks[mp_hands.HandLandmark.WRIST.value].x, hand_landmarks[mp_hands.HandLandmark.WRIST.value].y])\n",
    "\n",
    "\n",
    "    ### Pause/Play control ###\n",
    "    if index_tip[1]*100 < index_pip[1]*100 and middle_tip[1]*100 < middle_pip[1]*100 and ring_tip[1]*100 < ring_pip[1]*100 and pinky_tip[1]*100 < pinky_pip[1]*100 and thumb_tip[0]*100 < thumb_cmc[0]*100 and thumb_tip[0]*100 < thumb_ip[0]*100:\n",
    "        cv2.putText(image, \"Pause/Play\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.press(\"space\")\n",
    "            last_press_time = time.time()\n",
    "\n",
    "    ### Full screen control ### \n",
    "    elif index_tip[1]*100 > index_pip[1]*100 and middle_tip[1]*100 < middle_pip[1]*100 and ring_tip[1]*100 < ring_pip[1]*100 and pinky_tip[1]*100 < pinky_pip[1]*100 and index_tip[1]*100 < wrist[1]*100:\n",
    "        cv2.putText(image, \"Full Screen\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.press(\"f\")\n",
    "            last_press_time = time.time() \n",
    "            \n",
    "    ### Next video control ### \n",
    "    elif index_tip[1]*100 < index_pip[1]*100 and middle_tip[1]*100 < middle_pip[1]*100 and ring_tip[1]*100 > ring_pip[1]*100 and pinky_tip[1]*100 > pinky_pip[1]*100:\n",
    "        cv2.putText(image, \"Next Video\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.hotkey('shift', 'n')\n",
    "            last_press_time = time.time()    \n",
    "\n",
    "    ### Volume controls ###    \n",
    "    # volume down\n",
    "    elif index_tip[1]*100 > wrist[1]*100:\n",
    "        cv2.putText(image, \"Volume down\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.press(\"down\")\n",
    "            last_press_time = time.time()\n",
    "    \n",
    "    # volume up\n",
    "    elif index_tip[1]*100 < index_pip[1]*100 and middle_tip[1]*100 > middle_pip[1]*100 and ring_tip[1]*100 > ring_pip[1]*100 and pinky_tip[1]*100 > pinky_pip[1]*100:\n",
    "        cv2.putText(image, \"Volume Up\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.press(\"up\")\n",
    "            last_press_time = time.time()    \n",
    "    \n",
    "    # Mute/unmute\n",
    "    elif index_tip[1]*100 < index_pip[1]*100 and middle_tip[1]*100 > middle_pip[1]*100 and ring_tip[1]*100 > ring_pip[1]*100 and pinky_tip[1]*100 < pinky_pip[1]*100:\n",
    "        cv2.putText(image, \"Mute/Unmute\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.press(\"m\")\n",
    "            last_press_time = time.time()             \n",
    "    \n",
    "    ### Seek left control ### \n",
    "    elif thumb_tip[0]*100 < thumb_cmc[0]*100 and thumb_tip[0]*100 < thumb_ip[0]*100:\n",
    "        cv2.putText(image, \"Seek Left\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.press(\"left\")\n",
    "            last_press_time = time.time()\n",
    "\n",
    "    ### Seek right control ###  \n",
    "    elif index_tip[1]*100 > index_pip[1]*100 and middle_tip[1]*100 > middle_pip[1]*100 and ring_tip[1]*100 > ring_pip[1]*100 and pinky_tip[1]*100 < pinky_pip[1]*100:\n",
    "        cv2.putText(image, \"Seek right\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        if abs(time.time() - last_press_time) >= 1:\n",
    "            pyautogui.press(\"right\")\n",
    "            last_press_time = time.time()\n",
    "            \n",
    "  \n",
    "    \n",
    "# set up opencv to gather video stream, run hand landmark recogntion until the program is intrupted by user   \n",
    "with mp_hands.Hands(\n",
    "    max_num_hands=1,  # Only detect one hand\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display.\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        # Resize the image to 640x480 pixels\n",
    "        image = cv2.resize(image, (640, 480))\n",
    "\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get the landmarks for the first hand\n",
    "                hand_landmarks_1 = results.multi_hand_landmarks[0].landmark\n",
    "                hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                # Get the location of the wrist landmark\n",
    "                wrist = np.array([hand_landmarks_1[mp_hands.HandLandmark.WRIST.value].x, hand_landmarks_1[mp_hands.HandLandmark.WRIST.value].y])\n",
    "                # Get the location of the thumb joint\n",
    "                thumb_mcp = np.array([hand_landmarks_1[mp_hands.HandLandmark.THUMB_MCP.value].x, hand_landmarks_1[mp_hands.HandLandmark.THUMB_MCP.value].y])\n",
    "                # check if the thumb is located to the right or left of the wrist to determine right or left hand\n",
    "                if thumb_mcp[0] > wrist[0]:\n",
    "                    continue \n",
    "                else:\n",
    "                    # draw landmarks on the hand\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    #helper function to identify gestures\n",
    "                    Gesture_recognizer(hand_landmarks_1)\n",
    "            \n",
    "        # Display the image with the hand landmarks marked.\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "        # wait for escape or 'q' key press to stop the program\n",
    "        if cv2.waitKey(5) & 0xFF == 27 or cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "# close the program\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "for i in range (1,5):\n",
    "    cv2.waitKey(1)\n",
    "sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
